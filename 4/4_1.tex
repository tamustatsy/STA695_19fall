\documentclass{beamer}

\mode<presentation> {

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amsfonts}
\usepackage{mathrsfs, bbold}
\usepackage{amsmath,amssymb,graphicx}
\usepackage{mathtools} % gather

\def\boldsymbol{\sbf}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title["4"]{4: Asymptotic and connections to non-Bayesian approaches}

% \author{Taylor} 
% \institute[UVA] 
% {
% University of Virginia \\
% \medskip
% \textit{} 
% }
\date{09/16/19} 

\begin{document}
%----------------------------------------------------------------------------------------

\begin{frame}
\titlepage 
\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Introduction}

We examine what happens to posterior distributions when $n \to \infty$. These results help us understand our models better, and they can suggest useful approximations (when computation is too difficult).
\newline


\end{frame}
%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Bayesian Consistency}

A mathematical framework
\begin{enumerate}
\item likelihood we are using/assuming: $p(y \mid \theta)$
\item prior we are using $p(\theta)$
\item the true distribution $f(y) = \prod_{i=1}^n f(y_i)$
\item Kullback-Leibler divergence: $0 \le KL(\theta) = E_f\left[\log\left(\frac{ f(y_i) }{p(y_i \mid \theta) } \right) \right]$
\item $\theta_0$ is the {\bf unique} minimizer of $KL(\theta)$ 
\end{enumerate}

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Bayesian Consistency on finite parameter space}

\begin{block}{Theorem 1}
Suppose there exists $\theta_0$ such that $f(y_i) = p(y_i \mid \theta_0)$ and the parameter space is finite. If $p(\theta_0) > 0$ (prior puts mass on the true value), then
$$
p(\theta_0 \mid y) \to 1 
$$
as $n \to \infty$.
\end{block}
Convergence is with respect to $f(y)$!

\end{frame}
%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Bayesian Consistency }

Recall that if $\bar{Y}_n \overset{p}{\to} \mu < 0$, then $\sum_i Y_i \overset{p}{\to} -\infty$.
\newline

The $y_i$ are random here! We are keeping parameters fixed. Whenever $\theta \neq \theta_0$,
\begin{align*}
\frac{1}{n} \sum_{i=1}^n \log \left(\frac{p(y_i \mid \theta) }{p(y_i \mid \theta_0) } \right) &\overset{p}{\to} E_f\left[\log \left(\frac{p(y_i \mid \theta)f(y_i) }{p(y_i \mid \theta_0)f(y_i) } \right) \right] \\
&= KL(\theta_0) - KL(\theta) < 0
\end{align*}


\begin{enumerate}
\item so $\sum_{i=1}^n \log \left(\frac{p(y_i \mid \theta) }{p(y_i \mid \theta_0) } \right) \overset{p}{\to} -\infty$
\item so $\log \left(\frac{p(\theta \mid y)}{ p(\theta_0 \mid y) } \right) = \log \frac{p(\theta)}{p(\theta_0)} + \sum_{i=1}^n \log \left(\frac{p(y_i \mid \theta) }{p(y_i \mid \theta_0) } \right) \overset{p}{\to} -\infty$ if $p(\theta_0 ) > 0 $
\item so $\frac{p(\theta \mid y)}{ p(\theta_0 \mid y) }   \overset{p}{\to} 0$ as long as $p(\theta_0) > 0$
\item so $p(\theta_0 \mid y)  \overset{p}{\to} 1$ as long as $p(\theta_0) > 0$
\end{enumerate}

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Bayesian Consistency when the parameter space is compact}

\begin{block}{Theorem 2}
Suppose there exists $\theta_0$ such that $f(y_i) = p(y_i \mid \theta_0)$ and the parameter space is uncountable and compact. Let $A_{\epsilon} = \{ \theta \in \Theta : \rho(\theta, \theta_0 ) < \epsilon \}$ be the $\epsilon$-ball about $\theta_0$. For any $\epsilon > 0$, if $p(\theta \in A_{\epsilon}) > 0$, then
$$
p(\theta \in A_{\epsilon} \mid y) \to 1 
$$
as $n \to \infty$.
\end{block}
Convergence is with respect to $f(y)$!


\end{frame}



% %----------------------------------------------------------------------------------------
% \begin{frame}
% \frametitle{Bayesian Consistency }
% 
% Let $B$ be any set that doesn't contain $\theta_0$. There will be a finite number of these if we use compactness.
% \begin{align*}
% \frac{1}{n} \sum_{i=1}^n \log \left(\frac{ p(y_i \mid \theta \in B)}{  p(y_i \mid \theta' \in A_{\epsilon})  } \right) &\overset{p}{\to} E_f\left[ \log \left(\frac{ p(y_i \mid \theta \in B)}{  p(y_i \mid \theta' \in A_{\epsilon})  } \right)  \right] < 0\\
% \end{align*}
% % &= \int \log \left(\frac{ f(y)  }{  \int_{A_{\epsilon}} p(y \mid \theta') \text{d}\theta' } \right)f(y) \text{d}y \\
% % &-  \int \log \left(\frac{ f(y)  }{  \int_{B} p(y \mid \theta) \text{d}\theta } \right) f(y) \text{d}y < 0
% 
% 
% \begin{enumerate}
% \item so $\sum_{i=1}^n \log \left(\frac{ p(y_i \mid \theta \in B)}{  p(y_i \mid \theta' \in A_{\epsilon})  } \right) \overset{p}{\to} -\infty$
% \item so $\log \left(\frac{p(\theta \in B \mid y)}{ p(\theta \in A_{\epsilon} \mid y) } \right) = \log \frac{p(\theta \in B)}{p(\theta \in A_{\epsilon} )} + \sum_{i=1}^n \log \left(\frac{ p(y_i \mid \theta \in B)}{  p(y_i \mid \theta' \in A_{\epsilon})  } \right) \overset{p}{\to} -\infty$ if $p(\theta \in A_{\epsilon} ) > 0 $
% \item so $\frac{p(\theta \in B \mid y)}{ p(\theta \in A_{\epsilon} \mid y) }   \overset{p}{\to} 0$ as long as $p(\theta \in A_{\epsilon}) > 0$
% \item so $p(\theta \in A_{\epsilon} \mid y)  \overset{p}{\to} 1$ by compactness (finite number of other $B$s) and $p(\theta \in A_{\epsilon}) > 0$
% \end{enumerate}
% 
% \end{frame}
% %----------------------------------------------------------------------------------------
% \begin{frame}
% \frametitle{Bayesian Consistency }
% 
% I haven't been able to prove the following:
% \[
% E_f\left[ \log \left(\frac{ p(y_i \mid \theta \in B)}{  p(y_i \mid \theta' \in A_{\epsilon})  } \right)  \right] < 0
% \]
% 
% \end{frame}
% 
%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: Laplace's Method}

These ideas are based on using a Taylor approximation for your posterior distribution. 
\begin{enumerate}
\item approximations are second-order (quadratic)
\item centered about the {\bf posterior mode} $\hat{\theta}$
\item a better fit when the posterior is unimodal and symmetric
\item assume the mode is in the interior of the parameter space
\end{enumerate}
\pause

\begin{align*}
&\log p(\theta \mid y ) \approx \\
&\log p(\hat{\theta} \mid y) + \overbrace{(\theta - \hat{\theta})'\left[ \frac{\text{d}}{\text{d}\theta} \log p(\theta \mid y) \right] \bigg|_{\theta = \hat{\theta}}}^{0}  \\
&\hspace{5mm}  + \frac{1}{2}(\theta - \hat{\theta})'\left[\frac{\text{d}^2}{\text{d}\theta^2} \log p(\theta \mid y)\right] \bigg|_{\theta = \hat{\theta}}(\theta - \hat{\theta}) \\
&= c  - \frac{1}{2}(\theta - \hat{\theta})'\left[-\frac{\text{d}^2}{\text{d}\theta^2} \log p(\theta \mid y)\right] \bigg|_{\theta = \hat{\theta}}(\theta - \hat{\theta})
\end{align*}

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: Laplace's Method}

\[
\log p(\theta \mid y ) \approx 
c  - \frac{1}{2}(\theta - \underbrace{\hat{\theta} }_{\text{mean}} )' \underbrace{\left[-\frac{\text{d}^2}{\text{d}\theta^2} \log p(\theta \mid y)\right] \bigg|_{\theta = \hat{\theta}}}_{\text{precision} } (\theta - \hat{\theta})
\]

The {\bf observed posterior information} is
\begin{align*}
&\left[-\frac{\text{d}^2}{\text{d}\theta^2} \log p(\theta \mid y)\right] \bigg|_{\theta = \hat{\theta}} \\
&=
\left[-\frac{\text{d}^2}{\text{d}\theta^2} \log p(\theta)\right] \bigg|_{\theta = \hat{\theta}} +
\sum_{i=1}^n \left[-\frac{\text{d}^2}{\text{d}\theta^2} \log p(y_i \mid \theta)\right] \bigg|_{\theta = \hat{\theta}}\\
&= I(\hat{\theta})  
\end{align*}

$\hat{\theta}$ is interior point in the parameter space $\Rightarrow
I(\hat{\theta})$ is positive definite.
% So we have, approximately for large $n$, 
% \[
% \theta \mid (y_1, \ldots, y_n) \sim \text{Normal}\left(\hat{\theta} , [nJ(\theta_0)]^{-1} )\right)
% \]
% or
% \[
% \theta \mid y_1, \ldots, y_n \sim \text{Normal}\left(\hat{\theta} , [nJ(\hat{\theta})]^{-1} )\right
% \]


\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: Laplace's Method}

It's also justified to use the {\bf observed likelihood Fisher Information} $J(\theta) = -E\left(\frac{\text{d}^2 \log p(y \mid \theta) }{\text{d} \theta^2 } \right)$
\begin{align*}
&\left[-\frac{\text{d}^2}{\text{d}\theta^2} \log p(\theta \mid y)\right] \bigg|_{\theta = \hat{\theta}} \\
&=
\left[-\frac{\text{d}^2}{\text{d}\theta^2} \log p(\theta)\right] \bigg|_{\theta = \hat{\theta}} +
n \underbrace{\frac{1}{n}\sum_{i=1}^n \left[-\frac{\text{d}^2}{\text{d}\theta^2} \log p(y_i \mid \theta)\right] \bigg|_{\theta = \hat{\theta}}}_{ \text{approx. } J(\hat{\theta} )}
\end{align*}



\end{frame}


%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality}


So we have, approximately for large $n$,
\[
\theta \mid y_1, \ldots, y_n \sim \text{Normal}\left(\hat{\theta} , I(\hat{\theta})^{-1} )\right)
\]
or
\[
\theta \mid y_1, \ldots, y_n \sim \text{Normal}\left(\hat{\theta} , n^{-1}J(\hat{\theta})^{-1} )\right)
\]
\begin{enumerate}
\item $\hat{\theta}$ is the posterior mode. Using MLE (ignoring prior)
  can also be justified. 
\item $J(\hat{\theta})$ is the observed Fisher Information (of an individual datum's likelihood) evaluated at the posterior mode.
\item This result is known as the Bernstein-von Mises theorem when
  $\hat{\theta}$ is MLE. (Likelihood dominates prior in large sample)
\end{enumerate}
\end{frame}


\begin{frame}
  \frametitle{Asymptotic Normality --- Frequentist}
  Under some regularity conditions (notably that $\theta_0$ is not on
  the boundary of parameter space and posterior consistency), as $n \rightarrow \infty$, the
  posterior distribution of $\theta$, $p(\theta \mid y)$, approaches Normality with mean
  $\theta_0$ and variance $(n J(\theta_0))^{-1}$.
\end{frame}


%----------------------------------------------------------------------------------------
\begin{frame}
  \frametitle{Discussions about Normality approximation}
  \begin{itemize}
  \item Estimation of posterior mass via quantiles of $\chi^2$
    distribution 
\pause
  \item Data reduction and summary statistics, $\hat{\theta}$ and
    $I(\hat{\theta})$; useful in hierarchical modeling
\pause
\item Large sample confidence interval 
\[
 I(\hat{\theta})^{1/2}(\theta - \hat{\theta}) \mid y \sim N(0, I)
\]
\end{itemize}
\pause
Issues
\begin{itemize}
\item Cautious to use Normal approximation when the sample size is
  small
\pause
\item Cautious to use Normal approximation when the dimension of
  $\theta$ is high; typically more accurate for conditional
  and marginal distributions of components of $\theta$  
\pause
\item Convergence to normality of the posterior distribution can be
  dramatically improved by transformation on $\theta$ (example below)
  \end{itemize}
\end{frame}


%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: example}

Let $y_i \mid \mu, \theta \sim \text{N}(\mu, \exp(2\theta))$ and $p(\mu, \theta) \propto 1$ with $\theta = \log \sigma$. Then 
\begin{align*}
p(\mu, \theta \mid y) &\propto (2\pi)^{-n/2} \exp(-n\theta) \exp\left[-\frac{1}{2 \exp(2\theta) } \sum_i (y_i - \mu)^2 \right] \\
&= (2\pi)^{-n/2} \exp(-n\theta) \exp\left[-\frac{1}{2 \exp(2\theta) } \left\{ n(\mu - \bar{y})^2 + (n-1)s^2 \right\} \right]
\end{align*}
let's approximate this for some practice!

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: example}

\begin{align*}
&\frac{\text{d}}{\text{d} \mu} \log p(\mu, \theta \mid y) \\
&= \frac{\text{d}}{\text{d} \mu}\left[ -\frac{n}{2}\log(2\pi) -n\theta -\frac{1}{2 \exp(2\theta) } \left\{ n(\mu - \bar{y})^2 + (n-1)s^2 \right\} \right] \\
&=  -\frac{n(\mu - \bar{y})}{ \exp(2\theta) }  \overset{\text{set}}{=} 0 
\end{align*}
which means $\hat{\mu} = \bar{y}$

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: example}

\begin{align*}
&\frac{\text{d}}{\text{d} \theta} \log p(\mu, \theta \mid y) \\
&= \frac{\text{d}}{\text{d} \theta }\left[ -\frac{n}{2}\log(2\pi) -n\theta -\frac{1}{2 \exp(2\theta) } \left\{ n(\mu - \bar{y})^2 + (n-1)s^2 \right\} \right] \\
&=  -n + \left\{ n(\mu - \bar{y})^2 + (n-1)s^2 \right\} \exp(-2\theta) \overset{\text{set}}{=} 0 
\end{align*}
which means $\hat{\theta} = \log \left\{ \sqrt{ \frac{n-1}{n}s^2}  \right\}$ after we plug in $\hat{\mu}$

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: example}

The mean vector is
$$
\left[\begin{array}{c}
\hat{\mu} \\
\hat{\theta}
\end{array}\right]
= 
\left[\begin{array}{c}
\bar{y}\\
\log \left\{ \sqrt{ \frac{n-1}{n}s^2}  \right\}
\end{array}\right]
$$

Now let's find the observed (posterior) information

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: example}

\begin{align*}
&\frac{\text{d}^2}{\text{d} \mu^2} \log p(\mu, \theta \mid y) = - \frac{\text{d}}{\text{d} \mu}  \frac{n(\mu - \bar{y})}{ \exp(2\theta) }  \\
&= - n \exp(-2\theta)
\end{align*}

\begin{align*}
& \frac{\text{d}^2}{\text{d} \theta^2} \log p(\mu, \theta \mid y) =  \frac{\text{d}}{\text{d} \theta} \left\{ n(\mu - \bar{y})^2 + (n-1)s^2 \right\} \exp(-2\theta) \\
&=  -2 \left\{ n(\mu - \bar{y})^2 + (n-1)s^2 \right\} \exp(-2\theta) 
\end{align*}

\begin{align*}
& \frac{\text{d}^2}{\text{d} \mu \text{d} \theta} \log p(\mu, \theta \mid y) \\
&=  \frac{\text{d}}{\text{d} \mu} \left\{ n(\mu - \bar{y})^2 + (n-1)s^2 \right\} \exp(-2\theta) \\
&=  2 n(\mu - \bar{y})\exp(-2\theta) 
\end{align*}


\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Asymptotic Normality: example}

When we plug in the estimates, then the precision matrix is
$$
I(\hat{\theta} ) = 
-\frac{\text{d}^2}{\text{d}\theta^2} \log p(\theta \mid y) \bigg|_{\theta = \hat{\theta}} = 
\left[ \begin{array}{cc}
\frac{n^2}{(n-1)s^2} & 0 \\
0 & 2n
\end{array}\right]
$$
so
$$
p(\mu, \theta \mid y) \approx \text{N}\left(
\left[\begin{array}{c}
\bar{y}\\
\log \left\{ \sqrt{ \frac{n-1}{n}s^2}  \right\}
\end{array}\right],
\left[ \begin{array}{cc}
\frac{(n-1)s^2}{n^2} & 0 \\
0 & \frac{1}{2n}
\end{array}\right]
\right)
$$
\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Asymptotic Normality: Bioassay experiment}

\begin{verbatim}
w0 <- c(0,0)
optim_res <- optim(w0, bioassayfun, gr = NULL, df1, 
                   hessian = T)
w <- optim_res$par
S <- solve(optim_res$hessian)
\end{verbatim}

\url{http://avehtari.github.io/BDA_R_demos/demos_ch4/demo4_1.html}

\end{frame}




\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
