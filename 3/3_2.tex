\documentclass{beamer}

\mode<presentation> {

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amsfonts}
\usepackage{mathrsfs, bbold}
\usepackage{amsmath,amssymb,graphicx}
\usepackage{mathtools} % gather

\newcommand{\sbf}{\boldsymbol}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title["3"]{3: Introduction to multiparameter models}

% \author{Taylor} 
% \institute[UVA] 
% {
% University of Virginia \\
% \medskip
% \textit{} 
% }
\date{09/11/19} 

\begin{document}
%----------------------------------------------------------------------------------------

\begin{frame}
\titlepage 
\end{frame}


% ----------------------------------------------------------------------------------------
\begin{frame}
  \frametitle{Univariate Normal with unknown mean and variance}
Likelihood
\[
p(y \mid \mu, \sigma^2) = (\sigma^2)^{-n/2} \exp\left[ - \frac{1}{2\sigma^2}\left\{(n-1)  s^2 + n(\bar{y} - \mu)^2 \right\} \right]
\]
Prior
\[p(\mu, \sigma^2) = p(\sigma^2) p(\mu|\sigma^2) = \mbox{inv-gamma}(\nu_0/2, \sigma_0^2\nu_0/2) \mbox{N}(\mu_0, \sigma^2/\kappa_0)
\]
Posterior
\begin{align*}
  p(\mu, \sigma^2|y) = p(\sigma^2|y) p(\mu|\sigma^2,y) &= \mbox{inv-gamma}(\nu_n/2, \sigma_n^2\nu_n/2) \mbox{N}(\mu_n, \sigma^2/\kappa_n)\\
  &=\mbox{inverse-}\chi^2(\nu_n, \sigma_n^2) \mbox{N}(\mu_n, \sigma^2/\kappa_n)
\end{align*}

$p(\mu|y) = \mbox{t}_{\nu_n}(\mu_n, \sigma_n^2/\kappa_n)$
\end{frame}


% ----------------------------------------------------------------------------------------
\begin{frame}
  \frametitle{In-class discussion}
  (Wednesday Sep 18)

  \vspace{0.5cm}
  
  1. Investigate the role of priors in the univariate normal likelhood model using simulation.
  
  2. Investigate $p(\mu, \sigma^2|y_{new}, y_{old})$.

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Multivariate Normal Observations}

Let each observation $y_i$ follow a multivariate normal distribution. The likelihood $p(y_1, \ldots, y_n \mid \mu, \Sigma)$ is usefully written with a few properties of the trace operator:
\begin{align*}
&\propto \det(\Sigma)^{-n/2} \exp\left(-\frac{1}{2}\sum_i (y_i - \mu)'\Sigma^{-1}(y_i- \mu) \right) \\
&= \det(\Sigma)^{-n/2} \exp\left[-\frac{1}{2}\sum_i \text{tr}\{ (y_i - \mu)'\Sigma^{-1}(y_i- \mu) \} \right] \\
&= \det(\Sigma)^{-n/2} \exp\left[-\frac{1}{2}\sum_i \text{tr}\{ \Sigma^{-1}(y_i- \mu)(y_i - \mu)' \} \right] \\
&= \det(\Sigma)^{-n/2} \exp\left[-\frac{1}{2} \text{tr}\left\{\Sigma^{-1} \overbrace{\sum_i  (y_i- \mu)(y_i - \mu)'}^{S_0} \right\} \right]
\end{align*}

\end{frame}


%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Multivariate Normal Observations with known covariance matrix}

A conjugate prior for $p(y \mid \mu) \propto \det(\Sigma)^{-n/2} \exp\left[-\frac{1}{2} \text{tr}\left\{\overbrace{\Sigma^{-1}}^{ \text{known} } S_0 \right\} \right]$ is
$$
p(\mu \mid \mu_0, \Lambda_0) = \det(\Lambda_0)^{-1/2} \exp\left[(\mu - \mu_0)'\Lambda_0^{-1}(\mu - \mu_0) \right]
$$
This makes the posterior distribution (homework question exercise 3.13) normal with mean and precision
\[
\mu_n  = (\Lambda_0 + n \Sigma^{-1})^{-1}(\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \bar{y})
\]
\[
\Lambda_n^{-1} = \Lambda_0^{-1} + n \Sigma^{-1}.
\]
\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Multivariate Normal Observations with unknown covariance matrix}

When all of the elements of $\Sigma$ are unknown, we need a prior for that as well. This prior must put zero mass on matrices that aren't positive definite or aren't symmetric. 
\newline

A popular option is the {\bf inverse Wishart} distribution, which is analagous to the inverse-Gamma distribution. It has a degrees of freedom parameter: $\nu_0$. And it has a scale matrix parameter $\Lambda_0$. 
\newline
\pause

If $\Sigma \in \mathbb{R}^{d \times d}$, we will write
$$
\Sigma \sim \text{Inv-Wishart}_{\nu_0}(\Lambda_0^{-1})
$$
and we can write (something proportional to) the density as 
$$
p(\Sigma) \propto \det(\Sigma)^{-(\nu_0 + d + 1)/2}\exp\left( -\frac{1}{2} \text{tr}\left[\Lambda_0 \Sigma^{-1} \right] \right)
$$

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Multivariate Normal Observations with unknown covariance matrix}

The following is a conjugate prior
\begin{align*}
p(\mu \mid \Sigma)p(\Sigma) &= \text{N}(\mu_0, \Sigma / \kappa_0)\text{Inv-Wishart}_{\nu_0}(\Lambda_0^{-1}) \\
&\propto \left[\det(\Sigma)^{-1/2}\exp\left(-\frac{\kappa_0}{2}(\mu - \mu_0)'\Sigma^{-1}(\mu - \mu_0) \right) \right] \times \\
& \hspace{5mm} \left[   \det(\Sigma)^{-(\nu_0 + d + 1)/2}\exp\left( -\frac{1}{2} \text{tr}\left[\Lambda_0 \Sigma^{-1} \right] \right) \right]
\end{align*}

\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Multivariate Normal Observations with unknown covariance matrix}

Here's the posterior:
\begin{align*}
p(\mu, \Sigma \mid y) &\propto p(y \mid \mu, \Sigma)p(\mu \mid \Sigma)p(\Sigma) \\
&\propto \det(\Sigma)^{-n/2} \exp\left[-\frac{1}{2} \text{tr}\left\{\Sigma^{-1} \sum_i (\mu-y_i)(\mu-y_i)' \right\} \right] \times \\
& \hspace{5mm} \det(\Sigma)^{-1/2}\exp\left(-\frac{\kappa_0}{2}(\mu - \mu_0)'\Sigma^{-1}(\mu - \mu_0) \right)  \times \\
& \hspace{5mm}    \det(\Sigma)^{-(\nu_0 + d + 1)/2}\exp\left( -\frac{1}{2} \text{tr}\left[\Lambda_0 \Sigma^{-1} \right] \right) \\
&= \cdots
\end{align*}


\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Multivariate Normal Observations with unknown covariance matrix}
It helps to recognize $p(\mu \mid \Sigma, y)$ first, and then $p(\Sigma \mid y)$. Here is negative twice the log of the exponent:
\begin{align*}
&\text{tr}\left\{\Sigma^{-1} \sum_i (\mu-y_i)(\mu-y_i)'  + \kappa_0(\mu - \mu_0)'\Sigma^{-1}(\mu - \mu_0)  \right\} + c_1 \\
&= \text{tr} \left\{\Sigma^{-1} n(\mu - \bar{y}) (\mu - \bar{y})^T +
  \kappa_0 (\mu - \mu_0)(\mu - \mu_0)^T \right\} 
\\
&+ \text{tr}\left\{\Sigma^{-1} \sum_i (y_i - \bar{y})(y_i -
  \bar{y})^T\right\} + c_1\\
&= \text{tr} \left\{B (\mu - \mu_n)(\mu - \mu_n)^T\right\} +
  \text{tr}\left\{\Sigma^{-1} \frac{\kappa_0 n}{\kappa_0 + n} (\bar{y} - \mu_0)(\bar{y} -
  \mu_0)^T \right\} + c_2 \\
&= (\mu - \mu_n)'B(\mu - \mu_n) + c_3 
\end{align*}
where $B = (\Sigma/n)^{-1} + (\Sigma/\kappa_0)^{-1}$ and $\mu_n = B^{-1}\left[ (\Sigma/n)^{-1}\bar{y} +  (\Sigma/\kappa_0)^{-1}\mu_0 \right]$
\end{frame}

%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Multivariate Normal Observations with unknown covariance matrix}

Clearly 
$$
B = (\Sigma/n)^{-1} + (\Sigma/\kappa_0)^{-1} = \Sigma^{-1}(n + \kappa_0)
$$
and
$$
\mu_n = B^{-1}\left[ (\Sigma/n)^{-1}\bar{y} +  (\Sigma/\kappa_0)^{-1}\mu_0 \right] = \frac{\kappa_0 }{\kappa_0 + n }\mu_0 + \frac{n}{\kappa_0 + n}\bar{y}
$$

\end{frame}


%----------------------------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Multivariate Normal Observations with unknown covariance matrix}

Now $p(\Sigma|\mu, y)$. Denote $S = \sum_i (y_i - \bar{y})(y_i -
  \bar{y})^T$

\begin{align*}
p(\Sigma|\mu, y) \propto &\mbox{det}(\Sigma)^{-(\nu_0 + n + d + 1)/2}
  \\
  &\exp\left(-\frac{1}{2} \text{tr}\left[\Lambda_0 \Sigma^{-1} + S \Sigma^{-1} + \frac{\kappa_0 n}{\kappa_0 + n} (\bar{y} - \mu_0)(\bar{y} - \mu_0)^T \Sigma^{-1}\right] \right)
\end{align*}

$p(\Sigma|\mu, y) \sim \mbox{Inv-Wishart}_{\nu_n}(\Lambda_n)$

$\nu_n = \nu_0 + n$

$\Lambda_n = \Lambda_0  + S + \frac{\kappa_0 n}{\kappa_0 + n} (\bar{y} - \mu_0)(\bar{y} - \mu_0)^T$ 
\end{frame}


% %----------------------------------------------------------------------------------------
% \begin{frame}[fragile]
% \frametitle{A few notes on example 3.7}

% \begin{enumerate}
% \item It's a logistic regression model with two parameters: slope and intercept
% \item Groups: $i=1,2,3,4$
% \item For each group, sample size $n_i$ is known
% \item For each group, $y_i$ is a count (tumors, deaths, etc.)
% \item For each group, $x_i$ is a dose (continuous amount of treatment for each group)
% \end{enumerate}

% \end{frame}

% %----------------------------------------------------------------------------------------
% \begin{frame}[fragile]
% \frametitle{A few notes on example 3.7}

% For each group:
% \begin{gather*}
% y_i \mid \alpha, \beta \sim \text{Binomial}(n_i, \text{invlogit}(\alpha + \beta x_i))
% \end{gather*}

% We can write $\theta_i = \text{invlogit}(\alpha + \beta x_i)$ to make it cleaner, but note that this isn't introducing more parameters. The likelihood is
% $$
% p(y \mid \alpha, \beta) = \prod_{i=1}^4 \theta_i^{y_i}(1-\theta_i)^{n_i-y_i}
% $$


% \end{frame}

% %----------------------------------------------------------------------------------------
% \begin{frame}[fragile]
% \frametitle{A few notes on example 3.7}

% For each group:
% \begin{gather}
% y_i \mid \alpha, \beta \sim \text{Binomial}(n_i, \text{invlogit}(\alpha + \beta x_i))
% \end{gather}


% \begin{enumerate}
% \item The {\bf dose-response} is the relationship between $x_i$ and $\theta_i$ (which is assumed the same for each group $i$).
% \item {\bf LD-50} is the unknown quantity $-\alpha/\beta$. It only makes sense when $\beta > 0$, and it is the value of $x_i$ that yields $\theta_i = .5$ (plug it into eqn (1) above). Sometimes scientists are more interested in estimating this than they are in estimating individual parameters.
% \end{enumerate}

% \end{frame}


% %----------------------------------------------------------------------------------------
% \begin{frame}[fragile]
% \frametitle{A few notes on example 3.7}

% We assume an improper prior: $p(\alpha,\beta) \propto 1$. So
% \begin{align*}
% &\log p(\alpha,\beta \mid y) \\
% &= c + \log p(y \mid \alpha, \beta) + \log p(\alpha,\beta) \\
% &= c +  \log\prod_{i=1}^4 \theta_i^{y_i}(1-\theta_i)^{n_i-y_i} \\
% &= c +  \log\prod_{i=1}^4 \left(\frac{\theta_i}{(1-\theta_i)}\right)^{y_i}(1-\theta_i)^{n_i} \\
% &= c + \sum_{i=1}^4 y_i (\alpha + \beta x_i) - n_i\log[1 + \exp (\alpha + \beta x_i)] \\
% \end{align*}


% \begin{enumerate}
% \item One of the authors has provided \verb|R| code: \url{https://github.com/avehtari/BDA_R_demos/tree/master/demos_ch3}
% \end{enumerate}

% \end{frame}


\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
